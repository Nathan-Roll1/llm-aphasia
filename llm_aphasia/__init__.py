import torch
from typing import Optional, Dict

from .models import get_model_and_tokenizer
from .core import (
    get_target_modules,
    save_weights,
    restore_weights,
    apply_ablation_to_model,
    generate_text,
)

# Global state to manage ablations applied to cached models
_ABLATION_STATE: Dict[str, Dict] = {}

def ablate(
    model_path: str,
    ablation_strategy: str,
    severity: float,
    text_input: str,
    random_seed: int = 42,
    max_new_tokens: int = 50,
) -> str:
    """
    High-level wrapper to load a model, apply an ablation, and generate text.

    Manages model caching and state to ensure computational efficiency.

    Args:
        model_path (str): The name or path of the Hugging Face model.
        ablation_strategy (str): The ablation strategy to apply.
            Options: 'zero_out', 'mean_out', 'swap_x', 'swap_y', 
            'shuffle_x', 'shuffle_y', 'global', 'unablated'.
        severity (float): The degree of ablation, from 0.0 (none) to 1.0 (maximum).
        text_input (str): The prompt to send to the model.
        random_seed (int): Seed for reproducibility of random ablations.
        max_new_tokens (int): The maximum number of new tokens to generate.

    Returns:
        The text generated by the (potentially ablated) model.
    """
    if not (0.0 <= severity <= 1.0):
        raise ValueError("Severity must be between 0.0 and 1.0.")
    
    if ablation_strategy == 'unablated':
        severity = 0.0

    # 1. Get model from cache or load it
    model, tokenizer = get_model_and_tokenizer(model_path)
    
    # 2. Check current state and determine if changes are needed
    current_state = _ABLATION_STATE.get(model_path)
    new_state_is_different = (
        not current_state or
        current_state['strategy'] != ablation_strategy or
        current_state['severity'] != severity or
        current_state['seed'] != random_seed
    )

    if new_state_is_different:
        # Restore model to its original, pristine state if it was previously ablated
        if current_state and 'original_weights' in current_state:
            print("Restoring original weights...")
            restore_weights(current_state['original_weights'])

        # Apply new ablation if the strategy is not 'unablated'
        if severity > 0:
            print(f"Applying new ablation: strategy='{ablation_strategy}', severity={severity}")
            target_modules = get_target_modules(model)
            original_weights = save_weights(target_modules)
            generator = torch.Generator(device=model.device).manual_seed(random_seed)

            apply_ablation_to_model(
                target_modules=target_modules,
                strategy=ablation_strategy,
                severity=severity,
                generator=generator
            )
            
            # Store the new state
            _ABLATION_STATE[model_path] = {
                'strategy': ablation_strategy,
                'severity': severity,
                'seed': random_seed,
                'original_weights': original_weights,
            }
        else: # The new state is unablated, just clear the old state
            if model_path in _ABLATION_STATE:
                del _ABLATION_STATE[model_path]

    # 3. Model is now in the correct state, generate text
    output = generate_text(model, tokenizer, text_input, max_new_tokens)
    
    return output